Subcommand: cook
================

Purpose: Run Record_Pipeline with a YAML config (recipe).
Kitchen (Record_Pipeline) + Chef (HumanFn/RecordFn) already exist.
You write the Recipe (config YAML).

---

Config Format
=============

The config has two required top-level keys: `source_set_name` and `HumanRecords`
(inside `RecordArgs`).

```yaml
source_set_name: "OhioT1DM/@OhioT1DMxmlv250302"   # REQUIRED: Input SourceSet

HumanRecords:                                        # REQUIRED: at top level of config
    HmPtt:                                           #   or inside RecordArgs
        - Ptt
        - CGM5Min
        - Diet5Min
        - Exercise5Min
        - Med5Min

record_set_version: 0                                # Default: 0
```

The config may also be nested under `RecordArgs`:

```yaml
source_set_name: "OhioT1DM/@OhioT1DMxmlv250302"

RecordArgs:
  HumanRecords:
    HmPtt: ['Ptt', 'CGM5Min', 'Diet5Min', 'Exercise5Min', 'Med5Min']
  record_set_version: 0
  record_set_label: 1
  use_cache: false
  save_cache: true
```

**Required keys:** `source_set_name`, `HumanRecords`
**Optional keys:** `record_set_version` (default 0), `record_set_label`,
`use_cache`, `save_cache`

See `templates/config.yaml` for a fully annotated template.

---

How To Run
==========

**CLI command:**

```bash
source .venv/bin/activate
source env.sh
haistep-record --config config/test-haistep-ohio/2_test_record.yaml
```

**Test script:**

```bash
source .venv/bin/activate
source env.sh
python test/test_haistep/test_2_record/test_record.py \
    --config config/test-haistep-ohio/2_test_record.yaml
```

**Python API** (code/haipipe/record_base/record_pipeline.py):

```python
from haipipe.record_base import Record_Pipeline

config = {
    'HumanRecords': {'HmPtt': ['Ptt', 'CGM5Min', 'Diet5Min', 'Exercise5Min', 'Med5Min']},
    'record_set_version': 0
}
pipeline = Record_Pipeline(config, SPACE)

record_set = pipeline.run(
    source_set,                          # SourceSet (required)
    partition_index=None,                # 0-based partition index
    partition_number=None,               # Total partitions
    record_set_label=1,                  # Label for ID generation
    use_cache=True,                      # Load cached if available
    save_cache=True,                     # Save to cache
    profile=False                        # Return timing if True
)
```

**WRONG (cohort_name does NOT exist as a parameter):**

```python
# DO NOT DO THIS:
record_set = pipeline.run(source_set, cohort_name='OhioT1DM')
```

The RecordSet name is auto-generated by `_generate_record_set_name()` from
`source_set.source_set_name` and `config['record_set_version']`.

---

Available Chefs
================

**HumanFn (entity wrapper creators):**

```
HumanFn Name    | Description
----------------+-------------------------------------------
HmPtt           | Patient demographics wrapper (gender, age, disease type)
```

**RecordFn (record processors):**

```
RecordFn Name   | Description                         | Input SourceSet Table
----------------+-------------------------------------+----------------------
Ptt             | Patient time-indexed records         | PatientInfo
CGM5Min         | CGM readings, 5-min alignment        | CGM
Diet5Min        | Diet entries, 5-min alignment         | Diet
Exercise5Min    | Exercise entries, 5-min alignment     | Exercise
Med5Min         | Medication entries, 5-min alignment   | Medication
```

**Note:** The HumanRecords mapping is domain-configurable. The CGM domain
uses HmPtt with the five RecordFns above. Other domains (e.g., EHR) could
define different Human/Record combinations.

---

Naming Convention
==================

**Output asset naming:** auto-generated as `<RawDataName>_v<N>RecSet`
  - The raw data name is extracted from source_set_name by stripping the @SourceFnName
  - Example: source_set_name `"OhioT1DM/@OhioT1DMxmlv250302"` + version 0
    produces `"OhioT1DM_v0RecSet"`

**Store path:** `_WorkSpace/2-RecStore/<RecordSetName>/`

---

Verification
============

After running, **verify** the output:

1. **Check** output directory exists:
   `ls _WorkSpace/2-RecStore/<RecordSetName>/`

2. **Verify** FLAT directory structure:
   `ls _WorkSpace/2-RecStore/<RecordSetName>/`
   Should show: `Human-HmPtt/  Record-HmPtt.Ptt/  Record-HmPtt.CGM5Min/  ...`

3. **Check** manifest.json was created:
   `cat _WorkSpace/2-RecStore/<RecordSetName>/manifest.json`

4. **Load** and **inspect** (see load.md for pattern):

   ```python
   from haipipe.record_base import RecordSet
   record_set = RecordSet.load_from_disk(
       path='_WorkSpace/2-RecStore/OhioT1DM_v0RecSet', SPACE=SPACE
   )
   record_set.info()
   ```

---

Prerequisites
=============

Before running Record_Pipeline:

1. **Activate** .venv: `source .venv/bin/activate`
2. **Load** env.sh: `source env.sh`
3. Input SourceSet must exist at `_WorkSpace/1-SourceStore/<source_set_name>/`
4. If SourceSet does not exist, **run** Source_Pipeline first
   (see haipipe-data-1-source cook)

---

MUST DO
=======

- **Use** only registered HumanFn and RecordFn names (see Available Chefs)
- **Ensure** source_set_name matches an existing SourceSet
- **Follow** config key format exactly
- **Activate** .venv and **load** env.sh before running

---

MUST NOT
========

- **NEVER invent** HumanFn/RecordFn names that do not exist in code/haifn/fn_record/
- **NEVER skip** required config keys (source_set_name, HumanRecords)
- **NEVER pass** `cohort_name` to `pipeline.run()` -- does not exist
- **NEVER create** files in code/haifn/ -- **use** design-chef if you need a new RecordFn
