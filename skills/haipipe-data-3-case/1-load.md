Subcommand: load
================

Purpose: Inspect existing CaseSet assets (read-only).

Use this when you need to understand extracted features, verify trigger
coverage, check case counts, or debug feature quality issues.
This subcommand applies to any domain -- any CaseSet, any trigger type or feature set.

---

What To Read
============

CaseSets live under `_WorkSpace/3-CaseStore/`. CaseFn parquet files are
**@-prefixed and at the ROOT level** of the CaseSet directory.

```
_WorkSpace/3-CaseStore/
+-- <RecordSetName>/                     e.g., <CohortName>_v0RecSet
    +-- @v<N>CaseSet-<TriggerFolder>/
        +-- df_case.parquet              Main case table (EntityID, ObsDT, trigger cols)
        +-- df_lts.parquet               LTS segments (optional, for LTS triggers)
        +-- df_Human_Info.parquet        Entity metadata (optional)
        +-- @<CaseFnName1>.parquet       CaseFn features (at ROOT, NOT in subdirectory)
        +-- @<CaseFnName2>.parquet       Another CaseFn (at ROOT)
        +-- cf_to_cfvocab.json           Vocabulary per CaseFn
        +-- manifest.json
```

To see what actually exists:
```bash
ls _WorkSpace/3-CaseStore/                                   # RecordSets with cases
ls _WorkSpace/3-CaseStore/<RecordSetName>/                   # CaseSet versions
ls _WorkSpace/3-CaseStore/<RecordSetName>/@v<N>CaseSet-*/   # case files
```

**CRITICAL:**

- The main file is `df_case.parquet`, NOT `case_data.parquet`.
- CaseFn files are `@{CaseFnName}.parquet` at the ROOT level, NOT in a CaseFn/ subdirectory.
- Directory naming: `{RecSetName}/@v{N}CaseSet-{TriggerFolder}/`

---

Load API
========

```python
from haipipe.case_base import CaseSet

# Load existing CaseSet -- full path + SPACE
case_set = CaseSet.load_from_disk(
    path='_WorkSpace/3-CaseStore/<RecordSetName>/@v<N>CaseSet-<TriggerFolder>',
    SPACE=SPACE
)

# Load with selective CaseFn loading (faster, less memory)
case_set = CaseSet.load_from_disk(
    path='_WorkSpace/3-CaseStore/<RecordSetName>/@v<N>CaseSet-<TriggerFolder>',
    SPACE=SPACE,
    CaseFn_list=['<CaseFnName1>']         # Only load this CaseFn
)

# Alternative: load_asset also works
case_set = CaseSet.load_asset(
    path='_WorkSpace/3-CaseStore/<RecordSetName>/@v<N>CaseSet-<TriggerFolder>',
    SPACE=SPACE
)
```

**WRONG -- this API does NOT exist:**

```python
# WRONG: load_from_disk does NOT accept set_name= or store_key=
case_set = CaseSet.load_from_disk(set_name='...', store_key='...')
```

---

Inspect CaseSet Contents
========================

```python
# Base case data (df_case.parquet)
print('Number of cases:', len(case_set.df_case))
print('Case columns:', list(case_set.df_case.columns))
print('Case ID columns:', [c for c in case_set.df_case.columns if 'ID' in c or 'DT' in c])

# Per-CaseFn feature parquets (@CaseFnName.parquet)
for cf_name, cf_df in case_set.CaseFn_to_df.items():
    print(f'{cf_name}: {cf_df.shape[0]} rows, {cf_df.shape[1]} cols')
    print(f'  Columns: {list(cf_df.columns)[:10]}...')
    print()

# Check vocabulary
import json
with open(caseset_path + '/cf_to_cfvocab.json') as f:
    cf_vocabs = json.load(f)
for cf_name, vocab in cf_vocabs.items():
    print(f'{cf_name}: {len(vocab.get("tid2tkn", []))} tokens')
```

---

Quick Inspection Commands
=========================

```bash
# List available CaseSets
ls _WorkSpace/3-CaseStore/

# List trigger versions for a RecordSet
ls _WorkSpace/3-CaseStore/<RecordSetName>/

# List all files in a CaseSet (note: @-prefixed CaseFn files at root)
ls _WorkSpace/3-CaseStore/<RecordSetName>/@v<N>CaseSet-<TriggerFolder>/

# Check file sizes
du -sh _WorkSpace/3-CaseStore/<RecordSetName>/@v<N>CaseSet-<TriggerFolder>/*

# Read manifest
cat _WorkSpace/3-CaseStore/<RecordSetName>/@v<N>CaseSet-<TriggerFolder>/manifest.json
```

---

Inspection Checklist
====================

1. **Case count**: Total number of cases generated by trigger (rows in df_case.parquet)
2. **Case ID columns**: Verify PID, lts_id, ObsDT are present in df_case
3. **CaseFn coverage**: Each @CaseFn.parquet should have same number of rows as df_case
4. **Feature columns**: Check each CaseFn output has expected suffix columns (--tid, --wgt, --val, --str)
5. **Feature naming**: Verify `<Feature><Window>` convention (e.g., CGMValueBf24h)
6. **Temporal coverage**: Check ObsDT spans expected date range
7. **Split columns**: If patient_info_path was provided, check split_timebin exists in df_case
8. **Missing data**: Check for NaN/null ratios in feature columns
9. **Manifest**: Check manifest.json for lineage back to RecordSet
10. **Vocabulary**: Check cf_to_cfvocab.json for token counts per CaseFn

---

MUST DO
=======

1. **Activate .venv and source env.sh** before any Python inspection
2. **Check existence first** -- use `ls` or try/except before loading
3. **Use the correct API** -- `load_from_disk(path=..., SPACE=...)` or
   `load_asset(path=..., SPACE=...)`
4. **Look for CaseFn files at ROOT** -- `@CaseFnName.parquet`, not in a subdirectory
5. **Read df_case.parquet** as the primary case table -- not case_data.parquet

---

MUST NOT
========

- **NEVER modify** loaded data or any files in _WorkSpace/3-CaseStore/
- **NEVER assume** data exists -- check first with ls or try/except
- **NEVER load** without activating .venv and sourcing env.sh
- **NEVER look** for CaseFn files in a CaseFn/ subdirectory -- they are at ROOT with @ prefix
- **NEVER look** for case_data.parquet -- the file is df_case.parquet
