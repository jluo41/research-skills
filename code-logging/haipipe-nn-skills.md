──────────────────────────────────────────────────────────────────
❯ please read the skill in @Tools/plugins/research/skills I want
  you focus on the
    skills with haipipe-nn-** You might need to follow the example 
  like this Tools/plugins/research/skills/paper-incubator. 
  
  
  You      
  need read the code in code/haipipe/nn/ to understand the context.
  and also the code in @code/hainn/tsforecast to have a deep understanding of he context. also @code/hainn/tsforecast/instance_tsforecast.py, etc. 

  There are patterns here, and we wnat to keep the patterns consistent. 




ok, now you will be a very harsh reviewer, you just to be very tough, and be very skeptical about    
  your previous results. What you want to do is to compare with the code and this skill very very       
  carefully, and pickout the questions and problems, every time you find an issue, you will be highly   
  rewarded, your mindset isthe current method is the current skill is just shit, and you will enjoy to  
  find out the errors. So here you just pick out the problems and don't change any code. For each       
  issue, you will provide your commetns and why it is an error and you will also propose the            
  solutions.   



  I want this to be also kind of gneral, not only limited to the tefm or tsforecast, etc. I think it   
  should be applied to potential future model series as well.  



  don't be overfit, we need to unify the interface, not compatiable to every code,                     
                                        
● You're right. The skills should define the one correct interface -- prescriptive, not accommodating  
  every deviation. Let me simplify.                                             
                                                  

                                                  